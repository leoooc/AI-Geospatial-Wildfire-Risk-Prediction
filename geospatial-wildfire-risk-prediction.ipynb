{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11045317,"sourceType":"datasetVersion","datasetId":6880381}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T02:20:34.333843Z","iopub.execute_input":"2025-03-16T02:20:34.334153Z","iopub.status.idle":"2025-03-16T02:20:43.439380Z","shell.execute_reply.started":"2025-03-16T02:20:34.334128Z","shell.execute_reply":"2025-03-16T02:20:43.438440Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class WildfireDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        \"\"\"\n        image_dir: directory with input images.\n        mask_dir: directory with segmentation masks.\n        transform: torchvision transforms to apply (same for image and mask).\n        \"\"\"\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_list = sorted(os.listdir(image_dir))\n        self.mask_list = sorted(os.listdir(mask_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.image_dir, self.image_list[idx])\n        mask_path  = os.path.join(self.mask_dir, self.mask_list[idx])\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('L')  # grayscale for binary mask\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            mask = self.transform(mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        \n        # Binarize the mask: assuming values > 0 indicate the wildfire risk area.\n        mask = (mask > 0.5).float()\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T02:20:43.440263Z","iopub.execute_input":"2025-03-16T02:20:43.440633Z","iopub.status.idle":"2025-03-16T02:20:43.446922Z","shell.execute_reply.started":"2025-03-16T02:20:43.440600Z","shell.execute_reply":"2025-03-16T02:20:43.445895Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Residual Convolutional Block\nclass ResidualConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1   = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2   = nn.BatchNorm2d(out_channels)\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n\n    def forward(self, x):\n        residual = self.shortcut(x)\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        return out + residual  # learn residual function\n\n# Attention Gate for skip connections\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        \"\"\"\n        F_g: Number of channels in the gating (decoder) signal.\n        F_l: Number of channels in the skip (encoder) connection.\n        F_int: Number of intermediate channels.\n        \"\"\"\n        super().__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1),\n            nn.BatchNorm2d(F_int)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, g, x):\n        # g: gating signal (from decoder), x: features from encoder\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = F.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T02:20:43.447630Z","iopub.execute_input":"2025-03-16T02:20:43.447877Z","iopub.status.idle":"2025-03-16T02:20:43.479467Z","shell.execute_reply.started":"2025-03-16T02:20:43.447857Z","shell.execute_reply":"2025-03-16T02:20:43.478824Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# U-Net with Residual Blocks and Attention Gates\nclass UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n        super().__init__()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Encoder path\n        current_channels = in_channels\n        for feature in features:\n            self.downs.append(ResidualConvBlock(current_channels, feature))\n            current_channels = feature\n\n        # Bottleneck\n        self.bottleneck = ResidualConvBlock(features[-1], features[-1]*2)\n\n        # Decoder path\n        self.ups = nn.ModuleList()\n        self.attention_gates = nn.ModuleList()\n        for feature in reversed(features):\n            # Up-convolution (transpose conv)\n            self.ups.append(nn.ConvTranspose2d(features[-1]*2, feature, kernel_size=2, stride=2))\n            # Attention gate: gating signal channels = feature, skip connection channels = feature\n            self.attention_gates.append(AttentionGate(F_g=feature, F_l=feature, F_int=feature // 2))\n            # Decoder conv block; note that concatenation doubles channels (skip + upsampled)\n            self.ups.append(ResidualConvBlock(feature * 2, feature))\n            features[-1] = feature  # update for next iteration\n\n        # Final 1x1 conv to produce segmentation map\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n        # Encoder\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n        # Bottleneck\n        x = self.bottleneck(x)\n        # Reverse skip connections for decoder\n        skip_connections = skip_connections[::-1]\n        # Decoder\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)  # upsample\n            skip_connection = skip_connections[idx // 2]\n            # Apply attention gate to encoder features\n            attn = self.attention_gates[idx // 2](x, skip_connection)\n            # Concatenate along channel dimension\n            x = torch.cat((attn, x), dim=1)\n            x = self.ups[idx+1](x)  # conv block\n        return self.final_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T02:20:43.481271Z","iopub.execute_input":"2025-03-16T02:20:43.481541Z","iopub.status.idle":"2025-03-16T02:20:43.510921Z","shell.execute_reply.started":"2025-03-16T02:20:43.481521Z","shell.execute_reply":"2025-03-16T02:20:43.510120Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def iou_metric(pred, target, threshold=0.5, eps=1e-6):\n    pred = (pred > threshold).float()\n    target = (target > threshold).float()\n    intersection = (pred * target).sum(dim=[1,2,3])\n    union = (pred + target - pred * target).sum(dim=[1,2,3])\n    iou = (intersection + eps) / (union + eps)\n    return iou.mean()\n\ndef dice_coefficient(pred, target, threshold=0.5, eps=1e-6):\n    pred = (pred > threshold).float()\n    target = (target > threshold).float()\n    intersection = (pred * target).sum(dim=[1,2,3])\n    dice = (2 * intersection + eps) / (pred.sum(dim=[1,2,3]) + target.sum(dim=[1,2,3]) + eps)\n    return dice.mean()\n\ndef dice_loss(pred, target, threshold=0.5, eps=1e-6):\n    return 1 - dice_coefficient(pred, target, threshold, eps)\n\nclass BCEDiceLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bce = nn.BCEWithLogitsLoss()\n\n    def forward(self, pred, target):\n        bce_loss = self.bce(pred, target)\n        d_loss = dice_loss(torch.sigmoid(pred), target)\n        return bce_loss + d_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T02:20:43.512079Z","iopub.execute_input":"2025-03-16T02:20:43.512310Z","iopub.status.idle":"2025-03-16T02:20:43.537021Z","shell.execute_reply.started":"2025-03-16T02:20:43.512291Z","shell.execute_reply":"2025-03-16T02:20:43.536388Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def visualize_results(image, true_mask, pred_mask, prob_map):\n    \"\"\"\n    Visualizes:\n    1. A heatmap of the predicted probability map.\n    2. An overlay of the predicted binary mask on the original image.\n    3. An overlay of the ground truth mask on the original image.\n    \n    image: Tensor of shape (C, H, W)\n    true_mask: Tensor of shape (1, H, W)\n    pred_mask: Tensor of shape (1, H, W)\n    prob_map: Tensor of shape (1, H, W)\n    \"\"\"\n    image_np = image.permute(1, 2, 0).cpu().numpy()\n    true_mask_np = true_mask.squeeze().cpu().numpy()\n    pred_mask_np = pred_mask.squeeze().cpu().numpy()\n    prob_map_np = prob_map.squeeze().cpu().numpy()\n\n    plt.figure(figsize=(15,4))\n    \n    # Heatmap\n    plt.subplot(1, 3, 1)\n    plt.imshow(prob_map_np, cmap='hot')\n    plt.title('Prediction Heatmap')\n    plt.colorbar()\n    \n    # Overlay prediction: red for predicted risk area\n    plt.subplot(1, 3, 2)\n    overlay_pred = image_np.copy()\n    red_mask = np.zeros_like(image_np)\n    red_mask[..., 0] = 1  # red channel\n    overlay_pred = np.where(pred_mask_np[..., None] > 0.5, 0.5 * image_np + 0.5 * red_mask, image_np)\n    plt.imshow(overlay_pred)\n    plt.title('Overlayed Prediction')\n    \n    # Overlay ground truth: green for true wildfire regions\n    plt.subplot(1, 3, 3)\n    overlay_gt = image_np.copy()\n    green_mask = np.zeros_like(image_np)\n    green_mask[..., 1] = 1  # green channel\n    overlay_gt = np.where(true_mask_np[..., None] > 0.5, 0.5 * image_np + 0.5 * green_mask, image_np)\n    plt.imshow(overlay_gt)\n    plt.title('Ground Truth Overlay')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T02:20:43.537849Z","iopub.execute_input":"2025-03-16T02:20:43.538143Z","iopub.status.idle":"2025-03-16T02:20:43.565231Z","shell.execute_reply.started":"2025-03-16T02:20:43.538121Z","shell.execute_reply":"2025-03-16T02:20:43.564661Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs, device):\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    criterion = BCEDiceLoss()\n    best_val_iou = 0.0\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, masks in train_loader:\n            images = images.to(device)\n            masks = masks.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        avg_loss = running_loss / len(train_loader)\n        model.eval()\n        val_iou = 0.0\n        val_dice = 0.0\n        with torch.no_grad():\n            for images, masks in val_loader:\n                images = images.to(device)\n                masks = masks.to(device)\n                outputs = model(images)\n                probs = torch.sigmoid(outputs)\n                val_iou += iou_metric(probs, masks).item()\n                val_dice += dice_coefficient(probs, masks).item()\n        avg_val_iou = val_iou / len(val_loader)\n        avg_val_dice = val_dice / len(val_loader)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Val IoU: {avg_val_iou:.4f}, Val Dice: {avg_val_dice:.4f}\")\n\n        # Save best model\n        if avg_val_iou > best_val_iou:\n            best_val_iou = avg_val_iou\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n    print(\"Training complete. Best Val IoU:\", best_val_iou)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T02:20:43.565975Z","iopub.execute_input":"2025-03-16T02:20:43.566259Z","iopub.status.idle":"2025-03-16T02:20:43.588741Z","shell.execute_reply.started":"2025-03-16T02:20:43.566231Z","shell.execute_reply":"2025-03-16T02:20:43.588123Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def main():      \n    try:\n        import gdown\n    except ImportError:\n        os.system('pip install gdown')\n        import gdown\n\n    dataset_txt_url = \"https://drive.google.com/uc?id=1MVmx927A4AUZksHqb3liW4lUf5ts9l6v\"\n    labels_zip_url = \"https://drive.google.com/uc?id=1Gq0VXTElJWWfCcwnxLxA5CVWuls0is-y\"\n    \n    if not os.path.exists(\"dataset.txt\"):\n        gdown.download(dataset_txt_url, output=\"dataset.txt\", quiet=False)\n    if not os.path.exists(\"labels.zip\"):\n        gdown.download(labels_zip_url, output=\"labels.zip\", quiet=False)\n        os.system(\"unzip -oq labels.zip -d .\")\n        \n    image_paths = []\n    mask_paths = []\n    with open(\"dataset.txt\", \"r\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Assuming each line is \"image_path mask_path\" (space-separated or comma-separated)\n            parts = line.split()  # split by whitespace; use split(',') if comma-separated\n            if len(parts) >= 2:\n                img_path = parts[0]\n                mask_path = parts[1]\n                image_paths.append(img_path)\n                mask_paths.append(mask_path)\n\n    os.makedirs(\"data/images\", exist_ok=True)\n    os.makedirs(\"data/masks\", exist_ok=True)\n    # Move or copy files into these directories\n    for img, mask in zip(image_paths, mask_paths):\n        # os.rename or shutil.copy to move files\n        dest_img = os.path.join(\"data/images\", os.path.basename(img))\n        dest_mask = os.path.join(\"data/masks\", os.path.basename(mask))\n        if not os.path.exists(dest_img):\n            os.replace(img, dest_img)  # move image file\n        if not os.path.exists(dest_mask):\n            os.replace(mask, dest_mask)  # move mask file\n    # Update paths to point to the new locations\n    image_paths = [os.path.join(\"data/images\", os.path.basename(p)) for p in image_paths]\n    mask_paths  = [os.path.join(\"data/masks\",  os.path.basename(p)) for p in mask_paths]\n\n    from sklearn.model_selection import train_test_split\n    \n    # First split off a test set from the full dataset\n    train_imgs, test_imgs, train_masks, test_masks = train_test_split(\n        image_paths, mask_paths, test_size=0.2, random_state=42)\n    \n    # From the remaining training data, split out a validation set\n    train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n        train_imgs, train_masks, test_size=0.1, random_state=42)\n\n    \n    # Define transforms: resize images/masks and convert to tensor.\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor()\n    ])\n    \n    # Create datasets and dataloaders\n    train_dataset = WildfireDataset(train_imgs, train_masks, transform=transform)\n    val_dataset   = WildfireDataset(val_imgs, val_masks, transform=transform)\n    test_dataset  = WildfireDataset(test_imgs, test_masks, transform=transform)\n\n    \n    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n    val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False)\n    test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)\n    \n    # Use GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n    \n    # Initialize the model\n    model = UNet(in_channels=3, out_channels=1)\n    model = model.to(device)\n    \n    # Train the model\n    num_epochs = 20  # adjust as needed\n    train_model(model, train_loader, val_loader, num_epochs, device)\n    \n    # Load best model checkpoint for evaluation\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    \n    # Evaluate on test set and visualize results\n    with torch.no_grad():\n        for idx, (image, mask) in enumerate(test_loader):\n            image = image.to(device)\n            mask = mask.to(device)\n            output = model(image)\n            prob_map = torch.sigmoid(output)\n            pred_mask = (prob_map > 0.5).float()\n            print(f\"Visualizing Test Image {idx+1}\")\n            visualize_results(image[0].cpu(), mask[0].cpu(), pred_mask[0].cpu(), prob_map[0].cpu())\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}